{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5367/5367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 38ms/step - accuracy: 0.8349 - loss: 0.4141 - val_accuracy: 0.8792 - val_loss: 0.3002\n",
      "Epoch 2/10\n",
      "\u001b[1m5367/5367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 32ms/step - accuracy: 0.9096 - loss: 0.2427 - val_accuracy: 0.8803 - val_loss: 0.2989\n",
      "Epoch 3/10\n",
      "\u001b[1m5367/5367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m175s\u001b[0m 33ms/step - accuracy: 0.9299 - loss: 0.1890 - val_accuracy: 0.8813 - val_loss: 0.3239\n",
      "Epoch 4/10\n",
      "\u001b[1m5367/5367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 32ms/step - accuracy: 0.9520 - loss: 0.1368 - val_accuracy: 0.8753 - val_loss: 0.3614\n",
      "Epoch 5/10\n",
      "\u001b[1m5367/5367\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 32ms/step - accuracy: 0.9616 - loss: 0.1103 - val_accuracy: 0.8734 - val_loss: 0.4824\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.8819 - loss: 0.2937\n",
      "Combined Model Loss: 0.2989165186882019, Accuracy: 0.8803011178970337\n",
      "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NEG       0.92      0.89      0.90      4493\n",
      "         NEU       0.87      0.83      0.85      4254\n",
      "         POS       0.86      0.92      0.89      4670\n",
      "\n",
      "    accuracy                           0.88     13417\n",
      "   macro avg       0.88      0.88      0.88     13417\n",
      "weighted avg       0.88      0.88      0.88     13417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, LSTM, Embedding, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import Reshape\n",
    "\n",
    "# Tải các tài nguyên NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Bước 1: Tải dữ liệu\n",
    "df = pd.read_csv('data/merged_data.csv')\n",
    "\n",
    "# Bước 2: Mã hóa nhãn cảm xúc thành số\n",
    "label_encoder = LabelEncoder()\n",
    "df['label_encoded'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# Bước 3: Xóa các dòng có giá trị NaN trong cột 'content'\n",
    "df.dropna(subset=['content'], inplace=True)\n",
    "X = df['content']\n",
    "y = df['label_encoded']\n",
    "\n",
    "# Bước 4: Phân chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Bước 5: Tiền xử lý văn bản\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-ZÀ-ỹ\\s]', '', text)\n",
    "        tokens = text.split()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Tiến hành tiền xử lý dữ liệu\n",
    "X_train = X_train.apply(preprocess_text)\n",
    "X_test = X_test.apply(preprocess_text)\n",
    "\n",
    "# Bước 6: Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Chuyển đổi văn bản thành chuỗi số\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding để các chuỗi có cùng chiều dài\n",
    "maxlen = max([len(seq) for seq in X_train_seq])\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen, padding='post')\n",
    "\n",
    "# Chuyển đổi nhãn thành dạng one-hot\n",
    "y_train = to_categorical(y_train, num_classes=len(label_encoder.classes_))\n",
    "y_test = to_categorical(y_test, num_classes=len(label_encoder.classes_))\n",
    "\n",
    "# Thiết lập early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Xây dựng mô hình kết hợp CNN và LSTM\n",
    "model_combined = Sequential()\n",
    "model_combined.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=maxlen))\n",
    "model_combined.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model_combined.add(GlobalMaxPooling1D())\n",
    "model_combined.add(Reshape((1, 128)))  # Thay đổi kích thước cho LSTM\n",
    "model_combined.add(LSTM(128, return_sequences=False))\n",
    "model_combined.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "model_combined.compile(optimizer=Adam(learning_rate=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Huấn luyện mô hình kết hợp\n",
    "model_combined.fit(X_train_pad, y_train, epochs=10, batch_size=10, validation_data=(X_test_pad, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Đánh giá mô hình\n",
    "cnn_loss, cnn_acc = model_combined.evaluate(X_test_pad, y_test)\n",
    "print(f'Combined Model Loss: {cnn_loss}, Accuracy: {cnn_acc}')\n",
    "\n",
    "# Đánh giá chi tiết\n",
    "y_pred = model_combined.predict(X_test_pad)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(classification_report(y_test.argmax(axis=1), y_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "# Lưu mô hình\n",
    "model_combined.save('sentiment_model_combined.h5')\n",
    "\n",
    "# Lưu Tokenizer\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Lưu Label Encoder\n",
    "with open('label_encoder.pickle', 'wb') as handle:\n",
    "    pickle.dump(label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu sau khi làm sạch và lưu file thành công!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv  # Import thư viện để kiểm soát việc trích dẫn ký tự\n",
    "\n",
    "# Đọc dữ liệu từ file CSV với encoding phù hợp\n",
    "data = pd.read_csv('data/comments_data_ncds.csv', encoding='utf-8')\n",
    "\n",
    "# Chuyển thành DataFrame và chỉ lấy các cột cần thiết\n",
    "df = data[[\"content\", \"rating\"]].copy()\n",
    "\n",
    "# Gán nhãn\n",
    "df[\"label\"] = df[\"rating\"].apply(lambda x: \"POS\" if x >= 4 else (\"NEU\" if x == 3 else \"NEG\"))\n",
    "\n",
    "\n",
    "# Đổi tên cột rating thành start\n",
    "df.rename(columns={\"rating\": \"start\"}, inplace=True)\n",
    "\n",
    "# Xóa dấu \" trong nội dung một cách an toàn\n",
    "df[\"content\"] = df[\"content\"].str.replace(r\"[^a-zA-ZÀ-ỹ ]\", \"\", regex=True)\n",
    "\n",
    "# Loại bỏ dòng trống\n",
    "df = df[df[\"content\"].str.strip() != \"\"]\n",
    "\n",
    "# Loại bỏ dòng trùng lặp\n",
    "df = df.drop_duplicates()\n",
    "df = df[[\"content\", \"label\", \"start\"]]\n",
    "\n",
    "# Xuất dữ liệu sạch mà không có dấu ngoặc kép\n",
    "df.to_csv('data/cleaned_comments_data.csv', index=False, header=True, encoding='utf-8', quoting=csv.QUOTE_NONE, escapechar='\\\\')\n",
    "\n",
    "print(\"Dữ liệu sau khi làm sạch và lưu file thành công!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ghép file thành công và lưu vào merged_data.csv với tiêu đề cột! Cột 'start' đã chuyển về số nguyên.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Đọc hai file CSV, bỏ qua dòng tiêu đề nếu có\n",
    "df1 = pd.read_csv('data/cleaned_comments_data.csv', header=0, names=[\"content\", \"label\", \"start\"])\n",
    "df2 = pd.read_csv('data/data2.csv', header=0, names=[\"content\", \"label\", \"start\"])\n",
    "\n",
    "# Ghép hai DataFrame lại\n",
    "merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Loại bỏ dòng chứa tiêu đề nếu bị đọc vào dữ liệu\n",
    "merged_df = merged_df[merged_df[\"start\"] != \"start\"]\n",
    "\n",
    "# Loại bỏ dòng có giá trị 'start' không hợp lệ (không phải số)\n",
    "merged_df = merged_df[pd.to_numeric(merged_df[\"start\"], errors=\"coerce\").notna()]\n",
    "\n",
    "# Chuyển 'start' về số nguyên\n",
    "merged_df[\"start\"] = merged_df[\"start\"].astype(float).astype(int)\n",
    "\n",
    "# Lưu vào file mới với tiêu đề cột\n",
    "merged_df.to_csv('data/merged_data.csv', index=False, header=True, encoding='utf-8')\n",
    "\n",
    "print(\"Ghép file thành công và lưu vào merged_data.csv với tiêu đề cột! Cột 'start' đã chuyển về số nguyên.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "POS    23437\n",
      "NEG    22467\n",
      "NEU    21203\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('data/merged_data.csv', header=0, names=[\"content\", \"label\", \"start\"])\n",
    "print(df1['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu dữ liệu trung tính vào file: data\\neutral_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Mở rộng danh sách các cụm từ mang sắc thái trung tính\n",
    "neutral_adjectives = [\"bình thường\", \"ổn\", \"tạm được\", \"không tốt không xấu\", \"chấp nhận được\", \"trung bình\", \"khá ổn\", \"vừa phải\", \"thông thường\", \"đạt yêu cầu\"]\n",
    "neutral_nouns = [\"sản phẩm\", \"dịch vụ\", \"chất lượng\", \"giao hàng\", \"đóng gói\", \"giá cả\", \"cửa hàng\", \"website\", \"thái độ\", \"trải nghiệm\"]\n",
    "neutral_verbs = [\"là\", \"có\", \"khá\", \"cũng\", \"tương đối\", \"ở mức\", \"vẫn\", \"vừa\", \"thấy\", \"được\"]\n",
    "\n",
    "# Các mẫu câu trung tính\n",
    "sentence_templates = [\n",
    "    \"{noun} {verb} {adjective}.\",\n",
    "    \"{noun} này {verb} {adjective}.\",\n",
    "    \"Tôi thấy {noun} {verb} {adjective}.\",\n",
    "    \"{noun} {verb} {adjective} thôi.\"\n",
    "]\n",
    "\n",
    "# Tạo danh sách các câu đánh giá trung tính ngẫu nhiên\n",
    "neutral_reviews = []\n",
    "for _ in range(10000):\n",
    "    adjective = random.choice(neutral_adjectives)\n",
    "    noun = random.choice(neutral_nouns)\n",
    "    verb = random.choice(neutral_verbs)\n",
    "    template = random.choice(sentence_templates)\n",
    "    review = template.format(adjective=adjective, noun=noun, verb=verb)\n",
    "    neutral_reviews.append(review)\n",
    "\n",
    "# Tạo danh sách các label\n",
    "neu_labels = [\"NEU\"] * 10000\n",
    "\n",
    "# Tạo danh sách các giá trị start (3 sao)\n",
    "starts = [3] * 10000\n",
    "\n",
    "# Tạo DataFrame\n",
    "df_neutral = pd.DataFrame({\"content\": neutral_reviews, \"label\": neu_labels, \"start\": starts})\n",
    "\n",
    "# Lưu DataFrame vào file CSV\n",
    "output_dir = \"data\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "output_file = os.path.join(output_dir, \"neutral_reviews.csv\")\n",
    "df_neutral.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Đã lưu dữ liệu trung tính vào file: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu dữ liệu vào file: data\\negative_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Mở rộng danh sách các cụm từ mang sắc thái tiêu cực\n",
    "negative_adjectives = [\"tệ hại\", \"kém chất lượng\", \"thất vọng\", \"dở tệ\", \"bực mình\", \"không đáng tiền\", \"hỏng hóc\", \"chậm chạp\", \"lừa đảo\", \"phiền phức\", \"tồi tệ\", \"ghê tởm\", \"kinh khủng\", \"rác rưởi\", \"bỏ đi\", \"kém\", \"dở\", \"xấu\", \"xoàng\", \"thảm hại\"]\n",
    "negative_nouns = [\"sản phẩm\", \"dịch vụ\", \"chất lượng\", \"giao hàng\", \"đóng gói\", \"giá cả\", \"cửa hàng\", \"website\", \"thái độ\", \"trải nghiệm\", \"nhà sản xuất\", \"người bán\", \"chính sách\", \"hỗ trợ\", \"bảo hành\"]\n",
    "negative_verbs = [\"là\", \"có\", \"gây ra\", \"khiến tôi\", \"làm tôi\", \"không tốt\", \"không như mong đợi\", \"không hài lòng\", \"không đáng mua\", \"cần phải cải thiện\", \"thật\", \"quá\", \"rất\", \"không thể chấp nhận\", \"không nên mua\"]\n",
    "\n",
    "# Các mẫu câu (đã bỏ .capitalize())\n",
    "sentence_templates = [\n",
    "    \"{noun} {verb} {adjective}.\",\n",
    "    \"{noun} này thật {adjective}\",\n",
    "]\n",
    "\n",
    "# Các mẫu câu không cần capitalize\n",
    "sentence_templates_no_cap = [\n",
    "    \"Tôi rất {adjective} về {noun}.\",\n",
    "    \"Thật {adjective} khi {noun} lại {verb} như vậy.\",\n",
    "    \"Tôi không hiểu sao {noun} lại có thể {verb} đến thế.\"\n",
    "]\n",
    "\n",
    "# Tạo danh sách các câu đánh giá tiêu cực ngẫu nhiên\n",
    "negative_reviews = []\n",
    "for _ in range(10000):\n",
    "    adjective = random.choice(negative_adjectives)\n",
    "    noun = random.choice(negative_nouns)\n",
    "    verb = random.choice(negative_verbs)\n",
    "\n",
    "    # Chọn ngẫu nhiên mẫu câu\n",
    "    if random.choice([True, False]):\n",
    "        template = random.choice(sentence_templates)\n",
    "    else:\n",
    "        template = random.choice(sentence_templates_no_cap)\n",
    "\n",
    "    review = template.format(adjective=adjective, noun=noun, verb=verb)\n",
    "    negative_reviews.append(review)\n",
    "\n",
    "# Tạo danh sách các label\n",
    "neg_labels = [\"NEG\"] * 10000\n",
    "\n",
    "# Tạo danh sách các giá trị số sao ngẫu nhiên từ 1 đến 2\n",
    "ratings = [random.randint(1, 2) for _ in range(10000)]\n",
    "\n",
    "# Tạo DataFrame\n",
    "df = pd.DataFrame({\"content\": negative_reviews, \"label\": neg_labels, \"start\": ratings})\n",
    "\n",
    "# Lưu DataFrame vào file CSV\n",
    "output_dir = \"data\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "output_file = os.path.join(output_dir, \"negative_reviews.csv\")\n",
    "df.to_csv(output_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Đã lưu dữ liệu vào file: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
